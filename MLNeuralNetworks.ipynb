{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 475,
      "metadata": {
        "id": "CyPOwZa54XC0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as T\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "from typing import List\n",
        "from types import SimpleNamespace\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fill in your name and your partner's name below** (and name the `.ipynb` file correctly):\n",
        "\n",
        "---\n",
        "\n",
        "### YAP MING YANG\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "2LBbludOPdvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 4 - ML & Neural Networks  (DD1420)\n",
        "\n",
        "## Instructions\n",
        "\n",
        "This Jupyter Notebook contains exercises for DD1420 **Module 2: Machine Learning & Optimization**. To receive credit, all cells must be completed and must function correctly.\n",
        "\n",
        "### Collaboration\n",
        "\n",
        "- **Collaboration**: You are allowed to work on this exercise in pairs. However, each student must submit their own completed notebook. If you worked with a partner, please indicate your partner in the filename as described below.\n",
        "- **Code of Conduct**: Adhere to the following:\n",
        "  1. All group members are responsible for the work submitted.\n",
        "  2. Each student must honestly disclose any assistance received or external sources used.\n",
        "  3. Do not copy from other students' solutions. If you need help, use the appropriate Discussion Topic or sign up for a help session.\n",
        "\n",
        "### Completing the Exercise\n",
        "\n",
        "- **Code Cells**: Replace any placeholder comments like `YOUR CODE HERE` or \"YOUR ANSWER HERE\" with your own code or response. Once implemented, delete any instances of `raise NotImplementedError()`.\n",
        "- **Library Imports**: Do not import any additional libraries beyond those already included in the assignment.\n",
        "- **Derivations**: For derivation questions, you may use $\\LaTeX$ in markdown cells or upload an image of your handwritten derivation. In *Google Colab*, to upload an image, create a text cell, click the `Insert Image` icon, and upload your file.\n",
        "\n",
        "### Submission Instructions\n",
        "\n",
        "1. **Final Check**: Before submitting, ensure everything runs correctly by selecting `Runtime` -> `Restart and Run All`.\n",
        "2. **File Naming**: Download the notebook by selecting `File` -> `Download` -> `Download .ipynb`. Rename the file as follows:\n",
        "  - `Ex??_YOURLASTNAME_YOURFIRSTNAME_and_PARTNERLASTNAME_PARTNERFIRSTNAME.ipynb`\n",
        "  - Replace `??` with the correct exercise number. If you worked alone, omit the partner's name from the filename.\n",
        "3. **Submission**: Submit the `.ipynb` file to Canvas.\n",
        "\n",
        "### Oral Examination\n",
        "\n",
        "- **Preparation**: During the oral examination, you will be asked to demonstrate and explain your work. Have the link to your `.ipynb` file ready, with all cells executed.\n",
        "\n"
      ],
      "metadata": {
        "id": "6qAAcLodvTJP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.1 Training a neural network with backpropagation"
      ],
      "metadata": {
        "id": "mYu49AwLUR1j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise we will once again train a machine learning algorithm to classify handwritten digits from MNISt, although this time we will use a bigger version with $64 \\times 64$ images and 60,000 examples.\n",
        "\n",
        "<br>\n",
        "\n",
        "We will make use of [PyTorch](https://pytorch.org/) which is a popular machine learning framework for developing neural networks. You will not be expected to write PyTorch-specific code in this exercise, just standard numpy and Python. You won't need to follow along with what PyTorch is doing - we try to explain the important bits.\n",
        "\n",
        "<br>\n",
        "\n",
        "The first thing we will do is load the MNIST handwritten digits dataset,"
      ],
      "metadata": {
        "id": "S2KGtXp81JAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# these are pytorch's dataset class. it downloads and sets up the dataset\n",
        "mnist_train = torchvision.datasets.MNIST(root=\"data\",\n",
        "                                         train=True,\n",
        "                                         download=True,\n",
        "                                         transform=T.Compose([T.ToTensor(),\n",
        "                                                              T.Lambda(lambda x: torch.flatten(x)),\n",
        "                                                              ]),\n",
        "                                         )\n",
        "mnist_test = torchvision.datasets.MNIST(root=\"data\",\n",
        "                                        train=False,\n",
        "                                        download=True,\n",
        "                                        transform=T.Compose([T.ToTensor(),\n",
        "                                                             T.Lambda(lambda x: torch.flatten(x)),\n",
        "                                                             ]),\n",
        "                                        )"
      ],
      "metadata": {
        "id": "7DFdyBTG4d7i"
      },
      "execution_count": 476,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use something called a `dataloader` class from PyTorch, which handles feeding data from the dataset to the network. It does things like shuffle the data and yield samples.\n",
        "\n"
      ],
      "metadata": {
        "id": "PxHPxuKL2vd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# these are pytorch's dataloader class. they shuffle and yield sample points\n",
        "# for us. you don't have to code data manipulations yourself in this exercise\n",
        "train_loader = DataLoader(mnist_train, batch_size=1, shuffle=True)\n",
        "val_loader = DataLoader(mnist_test, batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "id": "oI6ND37D7Bph"
      },
      "execution_count": 477,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will define the size of our neural network, which will have $L = 3$ layers of width $m_{\\ell=\\{1,\\dots,L\\}} = \\{784, 30, 10\\}$"
      ],
      "metadata": {
        "id": "9RkrRYpLU6PB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters. sets up the network dimensions\n",
        "# how many nodes in each\n",
        "layers_list = [784, 30, 10]\n",
        "L = len(layers_list)"
      ],
      "metadata": {
        "id": "f2eq-qVn6iDM"
      },
      "execution_count": 478,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will define a function that will rescale our random weights to a distribution that has been proven to be a better initialization than just a standard `randn` distribution. The Xavier initialization simply divides the randomly initialized weights of each layer by the square root of the input size $\\sqrt{k}$."
      ],
      "metadata": {
        "id": "iQSlKHbhVj2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialization_scaling(weight_matrix):\n",
        "  \"\"\"\n",
        "  This implements Xavier initialization. It helps training of deep networks.\n",
        "  \"\"\"\n",
        "  input_size = weight_matrix.shape[0]\n",
        "  return weight_matrix / math.sqrt(input_size)"
      ],
      "metadata": {
        "id": "OsedR7cLViJx"
      },
      "execution_count": 479,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.1.1** *Complete the function provided below that creates a single layer of a neural network. Given the input size and the ouptut size this function should create the weight matrix and bias vector with random initialization. If the `scaled_init` flag is `True` it should apply the Xavier scaled defined above. The bias vector should be initialized to zero.*"
      ],
      "metadata": {
        "id": "EnvElKxUZkLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_single_layer(input_size: int, output_size: int, scaled_init=False):\n",
        "  \"\"\"\n",
        "  Create a single layer\n",
        "  output: a layer object. weights and biases are in layer.weight and layer.bias\n",
        "\n",
        "  Usage:\n",
        "    layer = create_single_layer(784, 10)\n",
        "    layer_weight_matrix = layer.weight\n",
        "    layer_bias_vector = layer.bias\n",
        "  \"\"\"\n",
        "  ## this makes sense, because the column multiply with rows of inputs\n",
        "  weight_matrix =  np.random.randn(output_size, input_size)\n",
        "  # Reshaping to 2D so that it doesn't cause problems later on\n",
        "  bias_vector = np.zeros(output_size).reshape(-1, 1)\n",
        "  if scaled_init:\n",
        "    weight_matrix = initialization_scaling(weight_matrix)\n",
        "\n",
        "  # The output being a SimpleNamespace is just a coding convenience. It\n",
        "  # emulates how we access the weights and biases in pytorch.\n",
        "  # without this we would have to access weights using layers[0][0],\n",
        "  # with this we can do layers[0].weight so that it is a bit more readable\n",
        "  return SimpleNamespace(weight=weight_matrix, bias=bias_vector)"
      ],
      "metadata": {
        "id": "wV8V6KLyclF4"
      },
      "execution_count": 480,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.1.2** *Complete the function provided below that creates all the layers of the neural network using the function you created in 4.1.1. The `layers_list` contains a list of layer widths for each layer. Loop through that list, create each layer, and append it to a `layers` list. The function should output `layers`, a list of initialized layers.*"
      ],
      "metadata": {
        "id": "nqGGTwudc8Yh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_layers(layers_list: List):\n",
        "  \"\"\"\n",
        "  Creates the weights and biases if layers according to the given list\n",
        "  output: a list of layers\n",
        "\n",
        "  Usage:\n",
        "    layers = create_layers(layers_list)\n",
        "    some_layer = layers[np.random.randint(L)]\n",
        "  \"\"\"\n",
        "\n",
        "  #To match the Lecture notation we start with idx=1, i.e. there is no W^(0) or b^(0)\n",
        "  layers = [None]\n",
        "  for idx in range(L-1):\n",
        "      # Input layer seems to be excluded\n",
        "      layer = create_single_layer(layers_list[idx], layers_list[idx+1])\n",
        "      layers.append(layer)\n",
        "  return layers"
      ],
      "metadata": {
        "id": "iDJCxziZf-8W"
      },
      "execution_count": 481,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we provide you with code for the logistic sigmoid activation function `sigmoid` and its derivative `sigmoid_prime` which should be applied to pre-activations."
      ],
      "metadata": {
        "id": "J8g38yBQgn1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "  \"\"\"\n",
        "  The sigmoid function\n",
        "  Usage:\n",
        "    activation_output = sigmoid(some_input)\n",
        "  \"\"\"\n",
        "  return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "  \"\"\"\n",
        "  $\\sigma\\prime$ in the lecture notes\n",
        "  \"\"\"\n",
        "  return sigmoid(z)*(1-sigmoid(z))"
      ],
      "metadata": {
        "id": "Bi3-sKWPgmaY"
      },
      "execution_count": 482,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.1.3** *Complete the following functions that the define the MSE loss (the loss for a single sample $\\ell_{MSE}$) and its derivative.*\n",
        "\n",
        "Note that this is not a very good choice of a loss for classification, but the derivative is easier to calculate and code, and it will give reasonable results on MNIST."
      ],
      "metadata": {
        "id": "2O4h6HCDhQBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(y_hat, y):\n",
        "  \"\"\"\n",
        "  This is the MSE loss for a single sample.\n",
        "  \"\"\"\n",
        "  return ((y_hat - y) ** 2).mean()\n",
        "\n",
        "\n",
        "def dloss_dyhat(y_hat, y):\n",
        "  \"\"\"\n",
        "  Gradient of the MSE loss for a single sample.\n",
        "  \"\"\"\n",
        "  return (2 * (y_hat - y))/y.size\n"
      ],
      "metadata": {
        "id": "tvPnL__SiCrG"
      },
      "execution_count": 483,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we will start to define some of the essential functions for training the neural network. We will start with a function for the pre-activations $z^{(\\ell)}$ and the activations $a^{(\\ell)}$."
      ],
      "metadata": {
        "id": "PEhUN0gcTwX6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.1.4**.  *Complete the function below that returns the preactivation given the layers weight matrix $W^{(\\ell)}$, bias vector $b^{(\\ell)}$ and the previous layers activation $a^{(\\ell-1)}$.*"
      ],
      "metadata": {
        "id": "P1KKtWK2UPT8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "hint: [The @ operator can be used as a shorthand for np.matmul on ndarrays.](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html)"
      ],
      "metadata": {
        "id": "OhfhG55u2QUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_preactivation(layer_weight, layer_bias, previous_activation):\n",
        "  # This step is extremely annoying.\n",
        "  # reshaping the layer_bias, because it is typically a 1-D\n",
        "  z = layer_weight @ previous_activation + layer_bias.reshape(-1, 1)\n",
        "  return z"
      ],
      "metadata": {
        "id": "5e_UMK6zVDcF"
      },
      "execution_count": 484,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A quick sanity check."
      ],
      "metadata": {
        "id": "68uWRZlJbDJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W = np.array([[2,-2], [-3,-1], [1, -3]])\n",
        "b = np.array([[-1], [0], [0]])\n",
        "a = np.array([[2],[-2]])\n",
        "calculate_preactivation(W,b,a)"
      ],
      "metadata": {
        "id": "rezS2g61aMZH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ce23dfd-e13e-4bd6-f976-ac5588c04ec2"
      },
      "execution_count": 485,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 7],\n",
              "       [-4],\n",
              "       [ 8]])"
            ]
          },
          "metadata": {},
          "execution_count": 485
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.1.5**.  *Complete the function below that returns the activation of a layer given the preactivation $z^{(\\ell)}$.*"
      ],
      "metadata": {
        "id": "UMqazgg6VTws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_activation(z):\n",
        "  return sigmoid(z)"
      ],
      "metadata": {
        "id": "TNJhCb0DVjik"
      },
      "execution_count": 486,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A quick sanity check."
      ],
      "metadata": {
        "id": "zlXsCHwkbVpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = np.array([[-1], [0], [1]])\n",
        "calculate_activation(z)"
      ],
      "metadata": {
        "id": "tYcI_AOebXpJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ca6fd5a-2c89-4ef5-c10b-eb68db9e01aa"
      },
      "execution_count": 487,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.26894142],\n",
              "       [0.5       ],\n",
              "       [0.73105858]])"
            ]
          },
          "metadata": {},
          "execution_count": 487
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.1.6.** *Write a function that computes and returns the gradient of the last layer $g^{(L)}$. You should make use of some previous functions you have written. You may need to use `.squeeze()` to ensure dimensions match.*"
      ],
      "metadata": {
        "id": "El433SRzWPNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_g_L(y, a, z):\n",
        "  dy_hat = dloss_dyhat(a, y).T\n",
        "  return dy_hat @ np.diag(sigmoid_prime(z).squeeze())"
      ],
      "metadata": {
        "id": "M07_HvchWstG"
      },
      "execution_count": 488,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A quick sanity check.\n"
      ],
      "metadata": {
        "id": "EBhWqT0ScoYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.ones((5,1))\n",
        "a = np.zeros((5,1))\n",
        "z = np.ones((5,1))\n",
        "calculate_g_L(y,a,z)\n"
      ],
      "metadata": {
        "id": "TMlqDte5A4Qo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c42ef4ad-c8a9-4737-8b1b-243d85d29fcc"
      },
      "execution_count": 489,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.07864477, -0.07864477, -0.07864477, -0.07864477, -0.07864477]])"
            ]
          },
          "metadata": {},
          "execution_count": 489
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.1.7.** *Write a function that computes and returns the gradient of an aribtrary layer $g^{(\\ell)}$ (except $L$). You may need to use `.squeeze()` to ensure dimensions match.*"
      ],
      "metadata": {
        "id": "JGTGO78VYHVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_g_l(g_l_previous, layer_weight_previous, z):\n",
        "  g_l = g_l_previous @ layer_weight_previous @ np.diag(sigmoid_prime(z).squeeze())\n",
        "  return g_l"
      ],
      "metadata": {
        "id": "T7vktrWkfX91"
      },
      "execution_count": 490,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A quick sanity check.\n"
      ],
      "metadata": {
        "id": "p3k8NDy3faGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.ones((5,1))\n",
        "a = np.zeros((5,1))\n",
        "z = np.ones((5,1))\n",
        "g_L = calculate_g_L(y,a,z)\n",
        "W = np.ones((5,5))\n",
        "z = np.ones((5,1))\n",
        "g_l = calculate_g_l(g_L,W,z)\n",
        "g_l"
      ],
      "metadata": {
        "id": "l1RNv59rftMQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7e1a38b-e92d-4aed-b47e-239e230caa8f"
      },
      "execution_count": 491,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.0773125, -0.0773125, -0.0773125, -0.0773125, -0.0773125]])"
            ]
          },
          "metadata": {},
          "execution_count": 491
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.1.8.** *Write a function that computes and returns the gradient of the loss w.r.t. the weights $\\nabla_{W^{(\\ell)}} J$ given the gradient $g^{(\\ell)}$ and the activations from the next layer $a^{(\\ell-1)}$.*"
      ],
      "metadata": {
        "id": "-oNk_Zn2iElA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_nabla_weight(g_l, a_next):\n",
        "  return g_l.T @ a_next.T"
      ],
      "metadata": {
        "id": "UXqKGG3Civgl"
      },
      "execution_count": 492,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A quick sanity check."
      ],
      "metadata": {
        "id": "Htx3_ABRjK8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g_l = np.ones((1,5))\n",
        "a_next = np.ones((3,1))\n",
        "nabW = calculate_nabla_weight(g_l, a_next)\n",
        "nabW"
      ],
      "metadata": {
        "id": "-b8j-Ti0jNGj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aa51b6a-8a98-493d-d58b-2f2c0224ba97"
      },
      "execution_count": 493,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.],\n",
              "       [1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 493
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.1.9.** *Write a function that computes and returns the gradient of the loss w.r.t. the bias $\\nabla_{b^{(\\ell)}} J$ given the gradient $g^{(\\ell)}$ and the activations from the next layer $a^{(\\ell-1)}$.*"
      ],
      "metadata": {
        "id": "mmJGMFxkj-W0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_nabla_bias(g_l):\n",
        "  return g_l.T"
      ],
      "metadata": {
        "id": "ZC2ohnICkNY0"
      },
      "execution_count": 494,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A quick sanity check"
      ],
      "metadata": {
        "id": "-WVHiit3kZuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g_l = np.ones((1,5))\n",
        "nabb = calculate_nabla_bias(g_l)\n",
        "nabb"
      ],
      "metadata": {
        "id": "vZeDF0pAkcqE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "802d3ebd-9640-493e-b5e8-3079a114a813"
      },
      "execution_count": 495,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 495
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! We are finally ready to put everything together and train the neural network. We have provided most of the code for you.\n",
        "\n",
        "<br>\n",
        "\n",
        "**4.1.10.** *Insert the functions you have defined in **4.1.2** and **4.1.5** through **4.1.9** in the correct places in the following code. Then run the code to train the neural network to classify handwritten digits.*"
      ],
      "metadata": {
        "id": "aXZokN08kw6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialization = create the layers\n",
        "layers = create_layers(layers_list)\n",
        "lr = 3.0\n",
        "# this is one epoch\n",
        "# ====== loop through 1 epoch using the data loader =====\n",
        "for (x, y) in tqdm(train_loader, desc=f'Epoch progress'):\n",
        "  # we used pytorch loaders, convert back to numpy and proper shape\n",
        "  x = x.numpy().T\n",
        "  # our network uses one_hot encoding to classify the digit so\n",
        "  # we convert the target to a one_hot encoding\n",
        "  y = torch.nn.functional.one_hot(y,num_classes=10).numpy().T\n",
        "  # actual updates occur here\n",
        "  activation = x\n",
        "  # set up the variables for saving the a & z values\n",
        "  # a^(0) is set to x\n",
        "  activations = [activation]\n",
        "  # z^(0) does not exist so we store a None for it\n",
        "  pre_activations = [None]\n",
        "\n",
        "  # ====== do forward propagation =====\n",
        "  # looping through the layers, computing and storing activations & pre-activations\n",
        "  for layer in layers[1:]: #since layers[0] does not exist/is None\n",
        "    pre_activation = calculate_preactivation(layer.weight, layer.bias, activation)\n",
        "    activation = calculate_activation(pre_activation)\n",
        "    # store these for the backprop\n",
        "    pre_activations.append(pre_activation)\n",
        "    activations.append(activation)\n",
        "\n",
        "  # ===== do backpropagation =====\n",
        "  # setting up record keeping of the gradients, nabla_weights and nabla_biases\n",
        "  # note that we have no gradients for W^(0) since we index starting at 1\n",
        "  g_ls = [None]\n",
        "  nabla_weights = [None]\n",
        "  nabla_biases = [None]\n",
        "  g_ls.extend([np.zeros((1, m)) for m in layers_list])\n",
        "  nabla_weights.extend([np.zeros_like(layer.weight) for layer in layers[1:]])\n",
        "  nabla_biases.extend([np.zeros_like(layer.bias) for layer in layers[1:]])\n",
        "\n",
        "  # looping through the layers, compute gradients and nabla's\n",
        "  for l in range(L-1, 0, -1):\n",
        "    # g^L is calculated differently, so we check and handle that first\n",
        "    if l == L-1:\n",
        "      g_ls[l] = calculate_g_L(y, activations[l], pre_activations[l])\n",
        "    else:\n",
        "      g_ls[l] = calculate_g_l(g_ls[l+1], layers[l+1].weight,pre_activations[l])\n",
        "    nabla_weights[l] = calculate_nabla_weight(g_ls[l], activations[l-1])\n",
        "    nabla_biases[l] = calculate_nabla_bias(g_ls[l])\n",
        "\n",
        "  # update the weights with the lr according to SGD algorithm\n",
        "  for l in range(L-1, 0, -1):\n",
        "    layers[l].weight = layers[l].weight- lr*nabla_weights[l]\n",
        "    layers[l].bias = layers[l].bias - lr*nabla_biases[l]"
      ],
      "metadata": {
        "id": "3pzRyGZ1njAN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0a5bd7e-31ee-4fbe-d452-253440f44188"
      },
      "execution_count": 496,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch progress: 100%|██████████| 60000/60000 [01:07<00:00, 885.17it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following code to make a forward pass over the test to see how it performed."
      ],
      "metadata": {
        "id": "KvLPS6AonYOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a forward function for evaluation, without backprop\n",
        "def forward(layers, x):\n",
        "  \"\"\"\n",
        "  Forward propagation.\n",
        "  \"\"\"\n",
        "  activation = x\n",
        "  for layer in layers[1:]: #since layers[0] does not exist\n",
        "    activation = sigmoid(layer.weight @ activation +layer.bias)\n",
        "  return activation\n",
        "\n",
        "# Evaluate\n",
        "test_results = []\n",
        "for (x,y) in val_loader:\n",
        "  x = x.numpy().T\n",
        "  y = y.numpy().T\n",
        "  test_results.append((np.argmax(forward(layers, x)), y))\n",
        "\n",
        "accuracy = sum(int(x == y) for (x, y) in test_results)/len(test_results)*100\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "VmowS071lWTf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1326039d-1840-4150-a8e6-a8559a6562b0"
      },
      "execution_count": 497,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90.47\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-497-032826d20837>:18: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  accuracy = sum(int(x == y) for (x, y) in test_results)/len(test_results)*100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we can visualize some of our predictions with the code below"
      ],
      "metadata": {
        "id": "U7gTXm5Sp9fR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# and show an example\n",
        "x, y = next(iter(val_loader))\n",
        "x = x.numpy().T\n",
        "y = y.numpy().T\n",
        "plt.title(f'Prediction is {np.argmax(forward(layers, x))}')\n",
        "plt.imshow(x.reshape(28,28), cmap='gray');"
      ],
      "metadata": {
        "id": "U0n-2NGxqAyv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "outputId": "e09010c3-a5b6-45de-bf41-b03167eecca5"
      },
      "execution_count": 498,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}
